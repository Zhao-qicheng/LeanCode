# 第一阶段训练脚本说明

## 📋 训练脚本清单

第一阶段需要训练 4 个 base 模型，对应 4 个训练脚本：

| 脚本名称 | 任务 | 模型 | tmux会话名 | 预计时间 |
|---------|------|------|-----------|---------|
| `start_training.sh` | CodeSearch | CodeBERT | `codesearch` | ~数小时 |
| `start_training_codesearch_codet5.sh` | CodeSearch | CodeT5 | `codesearch_codet5` | ~数小时 |
| `start_training_code2nl_codebert.sh` | Code2NL | CodeBERT | `code2nl_codebert` | ~1-2天 |
| `start_training_code2nl_codet5.sh` | Code2NL | CodeT5 | `code2nl_codet5` | ~1-2天 |

## 🚀 使用方法

### 启动训练

```bash
# 1. CodeSearch + CodeBERT (已在运行)
bash start_training.sh

# 2. CodeSearch + CodeT5
bash start_training_codesearch_codet5.sh

# 3. Code2NL + CodeBERT
bash start_training_code2nl_codebert.sh

# 4. Code2NL + CodeT5
bash start_training_code2nl_codet5.sh
```

### 查看所有训练会话

```bash
tmux ls
```

### 进入某个训练会话查看进度

```bash
# 进入 CodeSearch + CodeBERT 会话
tmux attach -t codesearch

# 进入 CodeSearch + CodeT5 会话
tmux attach -t codesearch_codet5

# 进入 Code2NL + CodeBERT 会话
tmux attach -t code2nl_codebert

# 进入 Code2NL + CodeT5 会话
tmux attach -t code2nl_codet5
```

**退出会话**: 按 `Ctrl+B` 然后按 `D`（会话会继续运行）

### 查看日志

```bash
# 查看最新的日志文件
ls -lt logs/

# 实时查看某个日志
tail -f logs/codesearch_training_XXXXXX.log
tail -f logs/codesearch_codet5_training_XXXXXX.log
tail -f logs/code2nl_codebert_training_XXXXXX.log
tail -f logs/code2nl_codet5_training_XXXXXX.log

# 查看错误信息
grep -i error logs/xxx.log
```

### 停止训练

```bash
# 停止某个训练任务
tmux kill-session -t codesearch
tmux kill-session -t codesearch_codet5
tmux kill-session -t code2nl_codebert
tmux kill-session -t code2nl_codet5
```

## 📊 训练进度监控

### 检查 GPU 使用情况

```bash
nvidia-smi
watch -n 1 nvidia-smi  # 每秒更新一次
```

### 查看进程状态

```bash
ps aux | grep run_classifier  # CodeSearch 任务
ps aux | grep run.py          # Code2NL CodeBERT 任务
ps aux | grep run_gen.py      # Code2NL CodeT5 任务
```

## 💡 并行训练建议

### GPU 资源充足（2块 RTX 4090）

如果想充分利用双 GPU，可以同时运行多个训练任务：

**推荐方案 1**（保守，避免 OOM）：
- 同时运行 2 个任务（每个任务使用 1 块 GPU）

**推荐方案 2**（激进，如果显存充足）：
- 同时运行 4 个任务（batch size 已调小）

### 按顺序训练（稳妥）

1. 先等待 `CodeSearch + CodeBERT` 完成
2. 依次运行其他三个任务

## 📁 训练输出位置

训练完成后，模型会保存在以下位置：

```
models/
├── codesearch/
│   ├── codebert/base/     # CodeSearch + CodeBERT
│   └── codet5/base/       # CodeSearch + CodeT5
└── code2nl/
    ├── codebert/base/     # Code2NL + CodeBERT
    └── codet5/base/       # Code2NL + CodeT5
```

## ⚠️ 注意事项

1. **数据准备**：确保已按 README.md 准备好所有数据集
2. **磁盘空间**：确保有足够空间存储模型和日志（建议至少 50GB）
3. **SSH 断开**：训练在 tmux 中运行，SSH 断开不影响训练
4. **Batch Size**：如遇到 OOM 错误，已经调小了 batch size（8/4）

## 🎯 下一步

第一阶段训练完成后：

1. ✅ 生成权重字典（如果训练时加了 `--gen_weight`）
2. ✅ 准备剪枝测试数据
3. ✅ 进行第二阶段的剪枝实验（80个实验）

## 📞 常见问题

**Q: 如何知道训练是否完成？**  
A: 查看日志文件末尾，会显示 "✓ 训练成功完成！" 或 "✗ 训练失败"

**Q: 训练中途中断了怎么办？**  
A: 重新运行对应的脚本，参数中有 `--overwrite_output_dir` 会从头开始

**Q: 可以修改训练参数吗？**  
A: 可以直接编辑对应的 `.sh` 脚本文件，修改参数后重新运行

**Q: 如何查看训练精度和损失？**  
A: 查看日志文件，或使用 TensorBoard（Code2NL + CodeT5 任务支持）

```bash
# 启动 TensorBoard
tensorboard --logdir=./models/code2nl/codet5/base/tensorboard
```

